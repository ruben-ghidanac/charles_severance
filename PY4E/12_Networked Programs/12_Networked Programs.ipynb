{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12: Networked Programs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While many of the examples in this book have focused on reading files and looking for data in those files, there are many different sources of information when one considers the Internet. In this chapter we will pretend to be a web browser and retrieve web pages using the HyperText Transfer Protocol (HTTP). Then we will read through the web page data and parse it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.1: HyperText Transfer Protocol - HTTP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network protocol that powers the web is actually quite simple and there is built-in support in Python called **sockets** which makes it very easy to make network connections and retrieve data over those sockets in a Python program.\n",
    "\n",
    "A ***socket*** is much like a file, except that a single socket provides a two-way connection between two programs. You can both read from and write to the same socket. If you write something to a socket, it is sent to the application at the other end of the socket. If you read from the socket, you are given the data which the other application has sent.\n",
    "\n",
    "But if you try to read a socket when the program on the other end of the socket has not sent any data, you just sit and wait. If the programs on both ends of the socket simply wait for some data without sending anything, they will wait for a very long time.\n",
    "\n",
    "So an important part of programs that communicate over the Internet is to have some sort of protocol. A protocol is a set of precise rules that determine: \n",
    "\n",
    "1. who is to go first, \n",
    "2. what they are to do,\n",
    "3. and then what the responses are to that message, and who sends next, and so on.\n",
    "\n",
    "In a sense the two applications at either end of the socket are doing a dance and making sure not to step on each other's toes.\n",
    "\n",
    "There are many documents which describe these network protocols. The HyperText Transfer Protocol is described in the following document:\n",
    "\n",
    "http://www.w3.org/Protocols/rfc2616/rfc2616.txt\n",
    "\n",
    "This is a long and complex 176-page document with a lot of detail. If you find it interesting, feel free to read it all. But if you take a look around page 36 of RFC2616 you will find the syntax for the GET request. \n",
    "\n",
    "To request a document from a web server, we make a connection to the `www.pr4e.org` server on port 80, and then send a line of the form `GET` http://data.pr4e.org/romeo.txt `HTTP/1.0`,\n",
    "where the second parameter is the web page we are requesting, and then we also send a blank line. The web server will respond with some header information about the document and a blank line followed by the document content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.2: The World's Simplest Web Browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps the easiest way to show how the HTTP protocol works is to write a very simple Python program that makes a connection to a web server and follows the rules of the HTTP protocol to request a document and display what the server sends back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 200 OK\n",
      "Date: Wed, 14 Aug 2024 04:41:48 GMT\n",
      "Server: Apache/2.4.52 (Ubuntu)\n",
      "Last-Modified: Sat, 13 May 2017 11:22:22 GMT\n",
      "ETag: \"a7-54f6609245537\"\n",
      "Accept-Ranges: bytes\n",
      "Content-Length: 167\n",
      "Cache-Control: max-age=0, no-cache, no-store, must-revalidate\n",
      "Pragma: no-cache\n",
      "Expires: Wed, 11 Jan 1984 05:00:00 GMT\n",
      "Connection: close\n",
      "Content-Type: text/plain\n",
      "\n",
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fair sun and kill the envious moon\n",
      "Who is already sick and pale with grief\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "\n",
    "# Create a socket object\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\n",
    "# Connect to the server at data.pr4e.org on port 80\n",
    "mysock.connect(('data.pr4e.org', 80))\n",
    "\n",
    "# Create the GET request command\n",
    "cmd = 'GET http://data.pr4e.org/romeo.txt HTTP/1.0\\r\\n\\r\\n'.encode()\n",
    "\n",
    "# Send the GET request to the server\n",
    "mysock.send(cmd)\n",
    "\n",
    "# Receive and print the response data in chunks of 20 bytes\n",
    "while True:\n",
    "    data = mysock.recv(20)\n",
    "    if len(data) < 1:\n",
    "        break\n",
    "    print(data.decode(), end='')\n",
    "\n",
    "# Close the socket connection\n",
    "mysock.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output shows both the HTTP headers and the body of the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of the code:\n",
    "\n",
    "1. Import the `socket` library: This library provides access to the BSD socket interface.\n",
    "2. Create a socket:\n",
    "```python\n",
    "# Create a socket object\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "```\n",
    "creates a new socket using `IPv4 (AF_INET)` and `TCP (SOCK_STREAM)`.\n",
    "\n",
    "3. Connect to the server: \n",
    "```python\n",
    "# Connect to the server at data.pr4e.org on port 80\n",
    "mysock.connect(('data.pr4e.org', 80))\n",
    "``` \n",
    "connects to the server at `data.pr4e.org` on **port** `80`, which is the standard port for **HTTP**.\n",
    "\n",
    "4. Create the **GET** request: \n",
    "```python\n",
    "# Create the GET request command\n",
    "cmd = 'GET http://data.pr4e.org/romeo.txt HTTP/1.0\\r\\n\\r\\n'.encode()\n",
    "```\n",
    "The GET request command is created as a string and encoded to bytes. Note the use of `\\r\\n` to denote the end of the HTTP request line and headers.\n",
    "\n",
    "5. Send the request: \n",
    "```python\n",
    "# Send the GET request to the server\n",
    "mysock.send(cmd)\n",
    "``` \n",
    "sends the encoded GET request to the server.\n",
    "\n",
    "6. Receive and print the response: \n",
    "```python\n",
    "while True:\n",
    "    data = mysock.recv(20)\n",
    "    if len(data) < 1:\n",
    "        break\n",
    "    print(data.decode(), end='')\n",
    "```\n",
    "The `while` loop reads data from the socket in chunks of `20` bytes and prints it. The loop breaks when no more data is received.\n",
    "\n",
    "7. Close the socket: \n",
    "```python\n",
    "# Close the socket connection\n",
    "mysock.close()\n",
    "``` \n",
    "closes the socket connection.\n",
    "\n",
    "By running this code, you will receive and print the contents of the `romeo.txt` file hosted at `data.pr4e.org`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`\\r` and `\\n` are control characters used to manage how text is displayed on screens and printed on paper. Here’s a detailed explanation:\n",
    "\n",
    "`\\r` - Carriage Return\n",
    "- **Symbol**: `\\r`\n",
    "- **ASCII Code**: 13 (0x0D in hexadecimal)\n",
    "- **Function**: Moves the cursor to the beginning of the line without advancing to the next line. Essentially, it \"returns the carriage\" (from typewriter terminology) to the start of the line.\n",
    "\n",
    "`\\n` - Line Feed (or Newline)\n",
    "- **Symbol**: `\\n`\n",
    "- **ASCII Code**: 10 (0x0A in hexadecimal)\n",
    "- **Function**: Moves the cursor down to the next line without returning to the beginning of the line. It advances the paper feed on a printer by one line.\n",
    "\n",
    "Historical Context and Usage\n",
    "- In older typewriters and printing systems, these two actions were often distinct:\n",
    "  - **Carriage Return (`\\r`)**: The print head (or carriage) would move back to the start of the line.\n",
    "  - **Line Feed (`\\n`)**: The paper would advance one line down.\n",
    "- Early computer systems and text terminals adopted these conventions.\n",
    "\n",
    "Combining `\\r` and `\\n` (`\\r\\n`)\n",
    "- **Usage**: Together, `\\r\\n` is used to denote the end of a line of text and the start of a new one.\n",
    "- **Reason**: It combines both actions—returning the carriage to the beginning of the line and moving the cursor (or paper) down to the next line.\n",
    "- **System Variations**:\n",
    "  - **Windows**: Uses `\\r\\n` to mark the end of a line (a legacy of early computer systems influenced by typewriter mechanics).\n",
    "  - **Unix/Linux**: Uses `\\n` alone to mark the end of a line.\n",
    "  - **Mac (pre-OS X)**: Used `\\r` alone, but modern macOS, which is Unix-based, uses `\\n`.\n",
    "\n",
    "Examples\n",
    "- **In Python**:\n",
    "  ```python\n",
    "  print(\"Hello\\rWorld\")\n",
    "  ```\n",
    "  - Output: `World` \n",
    "  \n",
    "  - The carriage return, brings the cursor back to the start, so \"World\" overwrites \"Hello\".\n",
    "\n",
    "  ```python\n",
    "  print(\"Hello\\nWorld\")\n",
    "  ```\n",
    "  - Output:\n",
    "    ```\n",
    "    Hello\n",
    "    World\n",
    "    ```\n",
    "  - The newline moves the cursor to the next line.\n",
    "\n",
    "  ```python\n",
    "  print(\"Hello\\r\\nWorld\")\n",
    "  ```\n",
    "  - Output:\n",
    "    ```\n",
    "    Hello\n",
    "    World\n",
    "    ```\n",
    "  - Carriage return moves the cursor to the start of the line, and newline moves it to the next line.\n",
    "\n",
    "In the Context of HTTP Headers\n",
    "- **Separation of Headers**: HTTP uses `\\r\\n` to separate each header line and uses a double sequence (`\\r\\n\\r\\n`) to indicate the end of the headers section.\n",
    "  ```http\n",
    "  GET / HTTP/1.1\\r\\n\n",
    "  Host: example.com\\r\\n\n",
    "  \\r\\n\n",
    "  ```\n",
    "  - The double `\\r\\n\\r\\n` sequence signifies the end of headers and the beginning of the body (if any).\n",
    "\n",
    "Using `\\r\\n` together ensures compatibility and clear separation of lines in network protocols like HTTP, maintaining consistency across different platforms and systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello\\rWorld\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "World\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello\\nWorld\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "World\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello\\r\\nWorld\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In HTTP protocol, the double `\\r\\n` sequence (`\\r\\n\\r\\n`) is used to signify the end of the HTTP headers and the beginning of the message body. Here's a breakdown of why this is necessary:\n",
    "\n",
    "1. **HTTP Request and Response Structure**:\n",
    "    - An HTTP request or response is composed of a start line, followed by headers, and then the optional message body.\n",
    "    - The headers provide metadata about the request or response and follow the start line.\n",
    "    - The message body contains the actual content being sent or received.\n",
    "\n",
    "2. **End of Headers**:\n",
    "    - Each header line ends with a carriage return and newline (`\\r\\n`).\n",
    "    - To indicate that there are no more headers and to separate the headers from the body, a blank line is used.\n",
    "    - A blank line in HTTP is represented by a carriage return and newline sequence with no characters in between (`\\r\\n`).\n",
    "\n",
    "3. **Double `\\r\\n` Sequence**:\n",
    "    - The double `\\r\\n` sequence is therefore used to mark the end of the headers section.\n",
    "    - The first `\\r\\n` is part of the last header line, and the second `\\r\\n` represents the blank line.\n",
    "\n",
    "Here's an example of an HTTP request showing the double `\\r\\n` clearly:\n",
    "\n",
    "```\n",
    "GET / HTTP/1.1\\r\\n\n",
    "Host: example.com\\r\\n\n",
    "User-Agent: Python-urllib/3.8\\r\\n\n",
    "Accept: */*\\r\\n\n",
    "\\r\\n\n",
    "```\n",
    "\n",
    "Explanation:\n",
    "\n",
    "- `GET / HTTP/1.1\\r\\n`: The request line.\n",
    "- `Host: example.com\\r\\n`: The Host header.\n",
    "- `User-Agent: Python-urllib/3.8\\r\\n`: The User-Agent header.\n",
    "- `Accept: */*\\r\\n`: The Accept header.\n",
    "- `\\r\\n`: The blank line indicating the end of the headers section.\n",
    "\n",
    "Similarly, for an HTTP response:\n",
    "\n",
    "```\n",
    "HTTP/1.1 200 OK\\r\\n\n",
    "Date: Tue, 28 Feb 2023 16:17:55 GMT\\r\\n\n",
    "Server: Apache/2.4.18 (Ubuntu)\\r\\n\n",
    "Content-Type: text/plain\\r\\n\n",
    "Content-Length: 167\\r\\n\n",
    "\\r\\n\n",
    "But, soft! what light through yonder window breaks?\n",
    "...\n",
    "```\n",
    "\n",
    "Explanation:\n",
    "\n",
    "- `HTTP/1.1 200 OK\\r\\n`: The status line.\n",
    "- `Date: Tue, 28 Feb 2023 16:17:55 GMT\\r\\n`: The Date header.\n",
    "- `Server: Apache/2.4.18 (Ubuntu)\\r\\n`: The Server header.\n",
    "- `Content-Type: text/plain\\r\\n`: The Content-Type header.\n",
    "- `Content-Length: 167\\r\\n`: The Content-Length header.\n",
    "- `\\r\\n`: The blank line indicating the end of the headers section.\n",
    "- `But, soft! what light through yonder window breaks?`: The body content.\n",
    "- `...`: The body content.\n",
    "\n",
    "In summary, the double `\\r\\n` (`\\r\\n\\r\\n`) sequence is essential in HTTP communications to clearly distinguish where the headers end and the body begins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we back to our program where we have to make a connection to the server.\n",
    "\n",
    "First the program makes a connection to **port** `80` on the **server** `www.py4e.com`. Since our program is playing the role of the \"web browser\", the HTTP protocol says we must send the **GET** command followed by a blank line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](./socket.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Socket Connection\n",
    "\n",
    "Once we send that blank line, we write a loop that receives data in chunks from the socket and prints the data out until there is no more data to read (i.e., the `recv()` returns an empty string).\n",
    "\n",
    "```python\n",
    "# Receive and print the response data in chunks of 20 bytes\n",
    "while True:\n",
    "    data = mysock.recv(20)\n",
    "    if len(data) < 1:\n",
    "        break\n",
    "    print(data.decode(), end='')\n",
    "```\n",
    "\n",
    "The output starts with headers which the web server sends to describe the document. For example, the `Content-Type` header indicates that the document is a plain text document (`text/plain`).\n",
    "\n",
    "```yaml\n",
    "Content-Type: text/plain\n",
    "```\n",
    "\n",
    "After the server sends us the headers, it adds a blank line `\\r\\n` to indicate the end of the headers, and then sends the actual data of the file `romeo.txt`.\n",
    "\n",
    "```yaml\n",
    "Content-Type: text/plain\n",
    "\\r\\n\n",
    "But soft what light through yonder window breaks\n",
    "It is the east and Juliet is the sun\n",
    "Arise fair sun and kill the envious moon\n",
    "Who is already sick and pale with grief\n",
    "```\n",
    "\n",
    "This example shows how to make a low-level network connection with sockets. Sockets can be used to communicate with a web server or with a mail server or many other kinds of servers. All that is needed is to find the document which describes the protocol and write the code to send and receive the data according to the protocol.\n",
    "\n",
    "However, since the protocol that we use most commonly is the HTTP web protocol, Python has a special library specifically designed to support the HTTP protocol for the retrieval of documents and data over the web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.3: Retrieving an image over HTTP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we retrieved a plain text file which had newlines in the file and we simply copied the data to the screen as the program ran. We can use a similar program to retrieve an image across using HTTP. Instead of copying the data to the screen as the program runs, we accumulate the data in a string, trim off the headers, and then save the image data to a file as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5120 5120\n",
      "5120 10240\n",
      "5120 15360\n",
      "5120 20480\n",
      "5120 25600\n",
      "5120 30720\n",
      "5120 35840\n",
      "5120 40960\n",
      "5120 46080\n",
      "5120 51200\n",
      "5120 56320\n",
      "5120 61440\n",
      "5120 66560\n",
      "5120 71680\n",
      "5120 76800\n",
      "5120 81920\n",
      "5120 87040\n",
      "5120 92160\n",
      "5120 97280\n",
      "5120 102400\n",
      "5120 107520\n",
      "5120 112640\n",
      "5120 117760\n",
      "5120 122880\n",
      "5120 128000\n",
      "5120 133120\n",
      "5120 138240\n",
      "5120 143360\n",
      "5120 148480\n",
      "5120 153600\n",
      "5120 158720\n",
      "5120 163840\n",
      "5120 168960\n",
      "5120 174080\n",
      "5120 179200\n",
      "5120 184320\n",
      "5120 189440\n",
      "5120 194560\n",
      "5120 199680\n",
      "5120 204800\n",
      "5120 209920\n",
      "5120 215040\n",
      "5120 220160\n",
      "5120 225280\n",
      "5120 230400\n",
      "208 230608\n",
      "Header length 394\n",
      "HTTP/1.1 200 OK\n",
      "Date: Sat, 03 Aug 2024 15:13:16 GMT\n",
      "Server: Apache/2.4.52 (Ubuntu)\n",
      "Last-Modified: Mon, 15 May 2017 12:27:40 GMT\n",
      "ETag: \"38342-54f8f2e5b6277\"\n",
      "Accept-Ranges: bytes\n",
      "Content-Length: 230210\n",
      "Vary: Accept-Encoding\n",
      "Cache-Control: max-age=0, no-cache, no-store, must-revalidate\n",
      "Pragma: no-cache\n",
      "Expires: Wed, 11 Jan 1984 05:00:00 GMT\n",
      "Connection: close\n",
      "Content-Type: image/jpeg\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "import time\n",
    "\n",
    "HOST = 'data.pr4e.org'\n",
    "PORT = 80\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "mysock.connect((HOST, PORT))\n",
    "mysock.sendall(b'GET http://data.pr4e.org/cover3.jpg HTTP/1.0\\r\\n\\r\\n')\n",
    "count = 0\n",
    "picture = b''\n",
    "\n",
    "while True:\n",
    "    data = mysock.recv(5120)\n",
    "    if (len(data) < 1): break\n",
    "    time.sleep(0.25)\n",
    "    count = count + len(data)\n",
    "    print(len(data), count)\n",
    "    picture = picture + data\n",
    "\n",
    "mysock.close()\n",
    "\n",
    "# Look for the end of the header (2 CRLF)\n",
    "pos = picture.find(b'\\r\\n\\r\\n')\n",
    "print('Header length', pos)\n",
    "print(picture[:pos].decode())\n",
    "\n",
    "# Skip past the header and save the picture data\n",
    "picture = picture[pos+4:]\n",
    "fhand = open('stuff.jpg', 'wb')\n",
    "fhand.write(picture)\n",
    "fhand.close()\n",
    "\n",
    "# Code: http://www.py4e.com/code3/urljpeg.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that for this url, the `Content-Type` header indicates that body of the document is an image (`image/jpeg`). Once the program completes, you can view the image data by opening the file `stuff.jpg` in an image viewer.\n",
    "\n",
    "As the program runs, you can see that we don't get 5120 characters each time we call the `recv()` method. We get as many characters as have been transferred across the network to us by the web server at the moment we call `recv()`.\n",
    "\n",
    "Your results may be different depending on your network speed. Also note that on the last call to `recv()` we get 208 bytes, which is the end of the stream, and in the next call to `recv()` we get a zero-length string that tells us that the server has called `close()` on its end of the socket and there is no more data forthcoming.\n",
    "\n",
    "We can slow down our successive `recv()` calls by uncommenting the call to `time.sleep()`. This way, we wait a quarter of a second after each call so that the server can \"get ahead\" of us and send more data to us before we call `recv()` again.\n",
    "\n",
    "There is a buffer between the server making `send()` requests and our application making `recv()` requests. When we run the program with the delay in place, at some point the server might fill up the buffer in the socket and be forced to pause until our program starts to empty the buffer. The pausing of either the sending application or the receiving application is called \"flow control\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.4: Retrieving web pages with urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we can manually send and receive data over HTTP using the socket library, there is a much simpler way to perform this common task in Python by using the `urllib` library.\n",
    "\n",
    "Using `urllib`, you can treat a web page much like a file. You simply indicate which web page you would like to retrieve and `urllib` handles all of the HTTP protocol and header details.\n",
    "\n",
    "The equivalent code to read the romeo.txt file from the web using urllib is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fair sun and kill the envious moon\n",
      "Who is already sick and pale with grief\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "fhand = urllib.request.urlopen('http://data.pr4e.org/romeo.txt')\n",
    "for line in fhand:\n",
    "    print(line.decode().strip())\n",
    "\n",
    "# Code: http://www.py4e.com/code3/urllib1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the web page has been opened with `urllib.urlopen`, we can treat it like a file and read through it using a `for` loop.\n",
    "\n",
    "When the program runs, we only see the output of the contents of the file. The headers are still sent, but the `urllib` code consumes the headers and only returns the data to us.\n",
    "\n",
    "As an example, we can write a program to retrieve the data for `romeo.txt` and compute the frequency of each word in the file as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'But': 1, 'soft': 1, 'what': 1, 'light': 1, 'through': 1, 'yonder': 1, 'window': 1, 'breaks': 1, 'It': 1, 'is': 3, 'the': 3, 'east': 1, 'and': 3, 'Juliet': 1, 'sun': 2, 'Arise': 1, 'fair': 1, 'kill': 1, 'envious': 1, 'moon': 1, 'Who': 1, 'already': 1, 'sick': 1, 'pale': 1, 'with': 1, 'grief': 1}\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "\n",
    "fhand = urllib.request.urlopen('http://data.pr4e.org/romeo.txt')\n",
    "\n",
    "counts = dict()\n",
    "for line in fhand:\n",
    "    words = line.decode().split()\n",
    "    for word in words:\n",
    "        counts[word] = counts.get(word, 0) + 1\n",
    "print(counts)\n",
    "\n",
    "# Code: http://www.py4e.com/code3/urlwords.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, once we have opened the web page, we can read it like a local file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.5: Parsing HTML and scraping the web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the common uses of the `urllib` capability in Python is to scrape the web. Web scraping is when we write a program that pretends to be a web browser and retrieves pages, then examines the data in those pages looking for patterns.\n",
    "\n",
    "As an example, a search engine such as Google will look at the source of one web page and extract the links to other pages and retrieve those pages, extracting links, and so on. Using this technique, Google *spiders* its way through nearly all of the pages on the web.\n",
    "\n",
    "Google also uses the frequency of links from pages it finds to a particular page as one measure of how \"important\" a page is and how high the page should appear in its search results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.6: Parsing HTML using regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One simple way to parse HTML is to use regular expressions to repeatedly search for and extract substrings that match a particular pattern.\n",
    "\n",
    "Here is a simple web page:\n",
    "\n",
    "```html\n",
    "<h3>The First Page</h3>\n",
    "If you like, you can switch to the\n",
    "<a href=\"http://www.dr-chuck.com/page2.htm\">\n",
    "Second Page</a>.\n",
    "</p>\n",
    "```\n",
    "\n",
    "We can construct a well-formed regular expression to match and extract the link values from the above text as follows:\n",
    "\n",
    "```python\n",
    "href=\"http://.+?\"\n",
    "```\n",
    "\n",
    "Our regular expression looks for strings that start with \"`href=\"http://`\", followed by one or more characters (\"`.+?`\"), followed by another double quote. The question mark added to the \"`.+?`\" indicates that the match is to be done in a \"non-greedy\" fashion instead of a \"greedy\" fashion. A non-greedy match tries to find the *smallest* possible matching string and a greedy match tries to find the *largest* possible matching string.\n",
    "\n",
    "We add parentheses to our regular expression to indicate which part of our matched string we would like to extract, and produce the following program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.dr-chuck.com/page2.htm\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "import re\n",
    "\n",
    "# Enter - http://www.dr-chuck.com/page1.htm\n",
    "url = input('Enter - ')\n",
    "html = urllib.request.urlopen(url).read()\n",
    "links = re.findall(b'href=\\\"(http://.*?)\\\"', html)\n",
    "for link in links:\n",
    "    print(link.decode())\n",
    "\n",
    "# Code: http://www.py4e.com/code3/urlregex.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.olin.edu\n",
      "http://greenteapress.com/\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "import re\n",
    "\n",
    "# Enter - http://allendowney.com/\n",
    "url = input('Enter - ')\n",
    "html = urllib.request.urlopen(url).read()\n",
    "links = re.findall(b'href=\\\"(http://.*?)\\\"', html)\n",
    "for link in links:\n",
    "    print(link.decode())\n",
    "\n",
    "# Code: http://www.py4e.com/code3/urlregex.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expressions work very nicely when your HTML is well formatted and predictable. But since there are a lot of \"broken\" HTML pages out there, a solution only using regular expressions might either miss some valid links or end up with bad data.\n",
    "\n",
    "This can be solved by using a robust HTML parsing library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.7: Parsing HTML using BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of Python libraries which can help you parse HTML and extract data from the pages. Each of the libraries has its strengths and weaknesses and you can pick one based on your needs.\n",
    "\n",
    "As an example, we will simply parse some HTML input and extract links using the BeautifulSoup library. You can download and install the *BeautifulSoup* code from:\n",
    "\n",
    "http://www.crummy.com/software/\n",
    "\n",
    "You can download and \"install\" BeautifulSoup or you can simply place the `BeautifulSoup.py` file in the same folder as your application.\n",
    "\n",
    "Even though HTML looks like XML and some pages are carefully constructed to be XML, most HTML is generally broken in ways that cause an XML parser to reject the entire page of HTML as improperly formed. BeautifulSoup tolerates highly flawed HTML and still lets you easily extract the data you need.\n",
    "\n",
    "We will use `urllib` to read the page and then use `BeautifulSoup` to extract the `href` attributes from the anchor (`a`) tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.dr-chuck.com/page2.htm\n"
     ]
    }
   ],
   "source": [
    "# To run this, you can install BeautifulSoup\n",
    "# https://pypi.python.org/pypi/beautifulsoup4\n",
    "\n",
    "# Or download the file\n",
    "# http://www.py4e.com/code3/bs4.zip\n",
    "# and unzip it in the same directory as this file\n",
    "\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "# http://www.dr-chuck.com/page1.htm\n",
    "url = input('Enter - ')\n",
    "html = urllib.request.urlopen(url, context=ctx).read()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('a')\n",
    "for tag in tags:\n",
    "    print(tag.get('href', None))\n",
    "\n",
    "# Code: http://www.py4e.com/code3/urllinks.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The program prompts for a web address, then opens the web page, reads the data and passes the data to the BeautifulSoup parser, and then retrieves all of the anchor tags and prints out the `href` attribute for each tag.\n",
    "\n",
    "You can use BeautifulSoup to pull out various parts of each tag as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TAG: <a href=\"http://www.dr-chuck.com/page2.htm\">\n",
      "Second Page</a>\n",
      "URL: http://www.dr-chuck.com/page2.htm\n",
      "Contents: \n",
      "Second Page\n",
      "Attrs: {'href': 'http://www.dr-chuck.com/page2.htm'}\n"
     ]
    }
   ],
   "source": [
    "# To run this, you can install BeautifulSoup\n",
    "# https://pypi.python.org/pypi/beautifulsoup4\n",
    "\n",
    "# Or download the file\n",
    "# http://www.py4e.com/code3/bs4.zip\n",
    "# and unzip it in the same directory as this file\n",
    "\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "# http://www.dr-chuck.com/page1.htm\n",
    "url = input('Enter - ')\n",
    "html = urlopen(url, context=ctx).read()\n",
    "\n",
    "# html.parser is the HTML parser included in the standard Python 3 library.\n",
    "# information on other HTML parsers is here:\n",
    "# http://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Retrieve all of the anchor tags\n",
    "tags = soup('a')\n",
    "for tag in tags:\n",
    "    # Look at the parts of a tag\n",
    "    print('TAG:', tag)\n",
    "    print('URL:', tag.get('href', None))\n",
    "    print('Contents:', tag.contents[0])\n",
    "    print('Attrs:', tag.attrs)\n",
    "\n",
    "# Code: http://www.py4e.com/code3/urllink2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These examples only begin to show the power of BeautifulSoup when it comes to parsing HTML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.8: Reading binary files using urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you want to retrieve a non-text (or binary) file such as an image or video file. The data in these files is generally not useful to print out, but you can easily make a copy of a URL to a local file on your hard disk using `urllib`.\n",
    "\n",
    "The pattern is to open the URL and use `read` to download the entire contents of the document into a string variable (`img`) then write that information to a local file as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "\n",
    "img = urllib.request.urlopen('http://data.pr4e.org/cover3.jpg').read()\n",
    "fhand = open('cover3.jpg', 'wb')\n",
    "fhand.write(img)\n",
    "fhand.close()\n",
    "\n",
    "# Code: http://www.py4e.com/code3/curl1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program reads all of the data in at once across the network and stores it in the variable `img` in the main memory of your computer, then opens the file `cover.jpg` and writes the data out to your disk. This will work if the size of the file is less than the size of the memory of your computer.\n",
    "\n",
    "However if this is a large audio or video file, this program may crash or at least run extremely slowly when your computer runs out of memory. In order to avoid running out of memory, we retrieve the data in blocks (or buffers) and then write each block to your disk before retrieving the next block. This way the program can read any size file without using up all of the memory you have in your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230210 characters copied.\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, urllib.parse, urllib.error\n",
    "\n",
    "img = urllib.request.urlopen('http://data.pr4e.org/cover3.jpg')\n",
    "fhand = open('cover3.jpg', 'wb')\n",
    "size = 0\n",
    "while True:\n",
    "    info = img.read(100000)\n",
    "    if len(info) < 1: \n",
    "        break\n",
    "    size = size + len(info)\n",
    "    fhand.write(info)\n",
    "\n",
    "print(size, 'characters copied.')\n",
    "fhand.close()\n",
    "\n",
    "# Code: http://www.py4e.com/code3/curl2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we read only 100,000 characters at a time and then write those characters to the `cover.jpg` file before retrieving the next 100,000 characters of data from the web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a Unix or Macintosh computer, you probably have a command built in to your operating system that performs this operation as follows:\n",
    "\n",
    "`curl -O http://www.py4e.com/cover.jpg`\n",
    "\n",
    "The command `curl` is short for \"copy URL\" and so these two examples are cleverly named `curl1.py` and `curl2.py` on www.py4e.com/code3 as they implement similar functionality to the `curl` command. There is also a `curl3.py` sample program that does this task a little more effectively, in case you actually want to use this pattern in a program you are writing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.E: Networked Programs (Exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the socket program `socket1.py` to prompt the user for the URL so it can read any web page. You can use `split('/')` to break the URL into its component parts so you can extract the host name for the socket `connect` call. Add error checking using `try` and `except` to handle the condition where the user enters an improperly formatted or non-existent URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# socket1.py\n",
    "\n",
    "import socket\n",
    "\n",
    "# Create a socket object\n",
    "mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\n",
    "# Connect to the server at data.pr4e.org on port 80\n",
    "mysock.connect(('data.pr4e.org', 80))\n",
    "\n",
    "# Create the GET request command\n",
    "cmd = 'GET http://data.pr4e.org/romeo.txt HTTP/1.0\\r\\n\\r\\n'.encode()\n",
    "\n",
    "# Send the GET request to the server\n",
    "mysock.send(cmd)\n",
    "\n",
    "# Receive and print the response data in chunks of 20 bytes\n",
    "while True:\n",
    "    data = mysock.recv(20)\n",
    "    if len(data) < 1:\n",
    "        break\n",
    "    print(data.decode(), end='')\n",
    "\n",
    "# Close the socket connection\n",
    "mysock.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 200 OK\n",
      "Date: Thu, 15 Aug 2024 13:02:37 GMT\n",
      "Server: Apache/2.4.52 (Ubuntu)\n",
      "Last-Modified: Sat, 13 May 2017 11:22:22 GMT\n",
      "ETag: \"a7-54f6609245537\"\n",
      "Accept-Ranges: bytes\n",
      "Content-Length: 167\n",
      "Cache-Control: max-age=0, no-cache, no-store, must-revalidate\n",
      "Pragma: no-cache\n",
      "Expires: Wed, 11 Jan 1984 05:00:00 GMT\n",
      "Connection: close\n",
      "Content-Type: text/plain\n",
      "\n",
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fair sun and kill the envious moon\n",
      "Who is already sick and pale with grief\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "\n",
    "# Function to parse the URL and extract the hostname and path\n",
    "def parse_url(url):\n",
    "    try:\n",
    "        # Remove the 'http://' or 'https://' prefix if present\n",
    "        if url.startswith('http://'):\n",
    "            url = url[7:]\n",
    "        elif url.startswith('https://'):\n",
    "            url = url[8:]\n",
    "\n",
    "        # Split the URL into hostname and path\n",
    "        host, path = url.split('/', 1)\n",
    "        path = '/' + path\n",
    "    except ValueError:\n",
    "        # If there's no '/', assume the entire URL is just the hostname\n",
    "        host = url\n",
    "        path = '/'\n",
    "\n",
    "    return host, path\n",
    "\n",
    "# Prompt the user for a URL\n",
    "url = input(\"Enter URL: \")\n",
    "\n",
    "try:\n",
    "    # Parse the URL to get the hostname and path\n",
    "    host, path = parse_url(url)\n",
    "\n",
    "    # Create a socket object\n",
    "    mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\n",
    "    # Try to connect to the server at the extracted hostname on port 80\n",
    "    mysock.connect((host, 80))\n",
    "\n",
    "    # Create the GET request command\n",
    "    cmd = f'GET {path} HTTP/1.0\\r\\nHost: {host}\\r\\n\\r\\n'.encode()\n",
    "\n",
    "    # Send the GET request to the server\n",
    "    mysock.send(cmd)\n",
    "\n",
    "    # Receive and print the response data in chunks of 512 bytes\n",
    "    while True:\n",
    "        data = mysock.recv(512)\n",
    "        if len(data) < 1:\n",
    "            break\n",
    "        print(data.decode(), end='')\n",
    "\n",
    "    # Close the socket connection\n",
    "    mysock.close()\n",
    "\n",
    "except socket.gaierror:\n",
    "    print(\"Error: Unable to resolve hostname or connect to server.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change your socket program so that it counts the number of characters it has received and stops displaying any text after it has shown 500 characters. The program should retrieve the entire document and count the total number of characters and display the count of the number of characters at the end of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP/1.1 200 OK\n",
      "Date: Thu, 15 Aug 2024 13:05:51 GMT\n",
      "Server: Apache/2.4.52 (Ubuntu)\n",
      "Last-Modified: Sat, 13 May 2017 11:22:22 GMT\n",
      "ETag: \"a7-54f6609245537\"\n",
      "Accept-Ranges: bytes\n",
      "Content-Length: 167\n",
      "Cache-Control: max-age=0, no-cache, no-store, must-revalidate\n",
      "Pragma: no-cache\n",
      "Expires: Wed, 11 Jan 1984 05:00:00 GMT\n",
      "Connection: close\n",
      "Content-Type: text/plain\n",
      "\n",
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fair sun and kill the envious moon\n",
      "Who \n",
      "\n",
      "Total characters received: 536\n"
     ]
    }
   ],
   "source": [
    "import socket\n",
    "\n",
    "# Function to parse the URL and extract the hostname and path\n",
    "def parse_url(url):\n",
    "    try:\n",
    "        # Remove the 'http://' or 'https://' prefix if present\n",
    "        if url.startswith('http://'):\n",
    "            url = url[7:]\n",
    "        elif url.startswith('https://'):\n",
    "            url = url[8:]\n",
    "\n",
    "        # Split the URL into hostname and path\n",
    "        host, path = url.split('/', 1)\n",
    "        path = '/' + path\n",
    "    except ValueError:\n",
    "        # If there's no '/', assume the entire URL is just the hostname\n",
    "        host = url\n",
    "        path = '/'\n",
    "\n",
    "    return host, path\n",
    "\n",
    "# Prompt the user for a URL\n",
    "url = input(\"Enter URL: \")\n",
    "\n",
    "try:\n",
    "    # Parse the URL to get the hostname and path\n",
    "    host, path = parse_url(url)\n",
    "\n",
    "    # Create a socket object\n",
    "    mysock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "\n",
    "    # Try to connect to the server at the extracted hostname on port 80\n",
    "    mysock.connect((host, 80))\n",
    "\n",
    "    # Create the GET request command\n",
    "    cmd = f'GET {path} HTTP/1.0\\r\\nHost: {host}\\r\\n\\r\\n'.encode()\n",
    "\n",
    "    # Send the GET request to the server\n",
    "    mysock.send(cmd)\n",
    "\n",
    "    # Initialize counters\n",
    "    total_characters = 0\n",
    "    displayed_characters = 0\n",
    "    max_display = 500\n",
    "\n",
    "    # Receive and process the response data\n",
    "    while True:\n",
    "        data = mysock.recv(512)\n",
    "        if len(data) < 1:\n",
    "            break\n",
    "\n",
    "        # Decode the received data\n",
    "        text = data.decode()\n",
    "\n",
    "        # Count the number of characters received so far\n",
    "        total_characters += len(text)\n",
    "\n",
    "        # Display the text only if the displayed characters are below the limit\n",
    "        if displayed_characters < max_display:\n",
    "            remaining_display = max_display - displayed_characters\n",
    "            if len(text) > remaining_display:\n",
    "                print(text[:remaining_display], end='')\n",
    "            else:\n",
    "                print(text, end='')\n",
    "\n",
    "            # Update the count of displayed characters\n",
    "            displayed_characters += len(text)\n",
    "\n",
    "    # Close the socket connection\n",
    "    mysock.close()\n",
    "\n",
    "    # Display the total number of characters received\n",
    "    print(\"\\n\\nTotal characters received:\", total_characters)\n",
    "\n",
    "except socket.gaierror:\n",
    "    print(\"Error: Unable to resolve hostname or connect to server.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `urllib` to replicate the previous exercise of (1) retrieving the document from a URL, (2) displaying up to 3000 characters, and (3) counting the overall number of characters in the document. Don't worry about the headers for this exercise, simply show the first 500 characters of the document contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But soft what light through yonder window breaks\n",
      "It is the east and Juliet is the sun\n",
      "Arise fair sun and kill the envious moon\n",
      "Who is already sick and pale with grief\n",
      "\n",
      "\n",
      "Total characters in document: 167\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "# Prompt the user for a URL\n",
    "url = input(\"Enter URL: \")\n",
    "\n",
    "try:\n",
    "    # Open the URL and retrieve the document\n",
    "    response = urllib.request.urlopen(url)\n",
    "    content = response.read()\n",
    "\n",
    "    # Decode the content to a string (assuming UTF-8 encoding)\n",
    "    text = content.decode('utf-8')\n",
    "\n",
    "    # Display up to the first 500 characters\n",
    "    print(text[:500])\n",
    "\n",
    "    # Display the total number of characters in the document\n",
    "    print(\"\\nTotal characters in document:\", len(text))\n",
    "\n",
    "except urllib.error.URLError as e:\n",
    "    print(f\"Error: {e.reason}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the `urllinks.py` program to extract and count paragraph (p) tags from the retrieved HTML document and display the count of the paragraphs as the output of your program. Do not display the paragraph text, only count them. Test your program on several small web pages as well as some larger web pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Advanced) Change the socket program so that it only shows data after the headers and a blank line have been received. Remember that `recv` is receiving characters (newlines and all), not lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12.G: Networked Programs (Glossary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **BeautifulSoup:** A Python library for parsing HTML documents and extracting data from HTML documents that compensates for most of the imperfections in the HTML that browsers generally ignore. You can download the BeautifulSoup code from www.crummy.com.\n",
    "- **port:** A number that generally indicates which application you are contacting when you make a socket connection to a server. As an example, web traffic usually uses port 80 while email traffic uses port 25.\n",
    "- **scrape:** When a program pretends to be a web browser and retrieves a web page, then looks at the web page content. Often programs are following the links in one page to find the next page so they can traverse a network of pages or a social network.\n",
    "- **socket:** A network connection between two applications where the applications can send and receive data in either direction.\n",
    "- **spider:** The act of a web search engine retrieving a page and then all the pages linked from a page and so on until they have nearly all of the pages on the Internet which they use to build their search index."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
